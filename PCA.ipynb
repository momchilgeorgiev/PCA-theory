{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c393f47-c330-48b4-9c8a-9695f6bf4d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from PIL import Image\n",
    "\n",
    "# plt global style\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f227b-3ccd-455c-8b44-4a87801a3aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions we will use later\n",
    "\n",
    "# Mean function with pd.sum\n",
    "def mean(x):\n",
    "    return x.sum(axis = None, numeric_only = True)/(len(x) - 1)\n",
    "\n",
    "# Standard diviation function\n",
    "def std(x):\n",
    "    variance = ((x - mean(x))**2).sum(numeric_only = True) / (len(x) - 1)\n",
    "    std_variation = np.sqrt(variance)\n",
    "    return std_variation\n",
    "\n",
    "# Standartize data by extracting the mean and deviding by std\n",
    "def standartize_data(x):\n",
    "    return (x - mean(x)) / std(x)\n",
    "\n",
    "\n",
    "# Function that calculates the covariance matrix of a given dataset\n",
    "def cov(x):\n",
    "    return (x.T @ x)/(x.shape[0]-1)\n",
    "\n",
    "# Function that gets only the numeric data from df\n",
    "def numeric_only_iris(x):\n",
    "    '''\n",
    "    The function takes only the numeric data\n",
    "    Acts just like \"numeric_only=\" from pd. \n",
    "    '''\n",
    "    return x.select_dtypes(include = np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466a208-beba-41c4-8695-6a252a5ec2f6",
   "metadata": {},
   "source": [
    "# PCA. Theory, uses and implementation.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8d24e-ba89-44bf-9586-24a3eb3cbcd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Work by: Momchil Georgiev\n",
    "\n",
    "###### Personal email: m.georgieff.public@gmail.com\n",
    "\n",
    "###### *Every work cited in this project has been acquired legally through public domains such as university websites, online publications and personal/company blogs.*\n",
    "\n",
    "***\n",
    "\n",
    "## Contents:\n",
    "\n",
    "#### 1. Project Motive\n",
    "#### 2. PCA and the theory behind it:\n",
    "\n",
    "    2.1. Standardize the data\n",
    "\n",
    "        2.1.1 Calculating the mean\n",
    "\n",
    "        2.1.2 Calculating the variance\n",
    "\n",
    "        2.1.3 Calculating standard deviation \n",
    "\n",
    "    2.2 Eigenvectors and eigenvalues of the covariance matrix\n",
    "\n",
    "    \n",
    "#### 3. Some other examples of PCA:\n",
    "\n",
    "    3.1 Breast cancer dataset\n",
    "    3.2 Eigenfaces\n",
    "\n",
    "#### 4. Conclusion\n",
    "    \n",
    "#### 5. References\n",
    "\n",
    "#### 6. Bibliography\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffb97b-12c3-4e7f-8a46-a2153589b630",
   "metadata": {},
   "source": [
    "## 1. Project motive\n",
    "During my course in data science I had the pleasure to work on a short project about PCA. I wanted to share what I have done and perhaps \"shine a little light\" on the subject while giving my own spin. \n",
    "\n",
    "This notebook is mainly designed for people who have **just started** studying programming and data science. I have tried to explain everything as simple as I can and if it gets technical - there are always sources to read that explain it in more detail.\n",
    "\n",
    "With that out of the way. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f419a27-069d-486b-9095-5195f5f346a7",
   "metadata": {},
   "source": [
    "## 2. PCA and the theory behind it\n",
    "\n",
    "<div style = \"text-align:center\">\n",
    "    <img src = \"./Media/ezgif-3-b43a62758a.gif\" width=\"20%\" height=\"20%\">\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "Principal component analysis (PCA) put simply is an algorithm used for dimensionality reduction of data. The way it works is by projecting the n-dimensional data to smaller dimensions, say from 10D to 3D while trying to preserve most of the information. It is important to mention that *information* in this sense represents the variance between variables.\n",
    "\n",
    "> The way I understand it, information is not so easy to define as it may seem at first. One way to think of information is variance: if you have a table with five athletes and their average speed in different elevations (0, 100 and 500m) and every single one of them has the same speed of 13 km/h it doesn't say much. But if they are different you start to see how every one of them does at those elevations. \n",
    "\n",
    "PCA is used a lot in statistics, medicine and chemistry for visualization, data optimization and cost reduction when doing sampling so one can guess it is very useful. Now let's see how it is made.\n",
    "\n",
    "For starters we need to mention that there are a lot of ways to implement PCA (single value decomposition, kernel PCA, sparse PCA and many more)[<sup>\\[1\\]</sup>](#fn1). In this project we will be going with single value decomposition (SVD for short). This means we have to:\n",
    "\n",
    "1. Standardize the data and find the covariance matrix\n",
    "2. Find the eigenvalues and eigenvectors of the cov. matrix\n",
    "3. Project the variables onto the principal components\n",
    "\n",
    "### 2.1 Standardize the data\n",
    "First thing we have to do is to standardize the data since PCA is very sensitive to large differences in values. While we are here we will center the mean at 0 for ease of use in the future. To begin we need to:\n",
    "\n",
    "1. Calculate the mean of the data\n",
    "2. Calculate the variance\n",
    "3. Calculate the standard deviation\n",
    "\n",
    "For the purposes of our explanation we will use the standard `iris` dataset. Let's import it, name it $D$ and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1c130-c72b-4fbe-844c-88feb8dbf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\")\n",
    "\n",
    "# Set a copy of the dataset so we can maniplute it freely without messing up the original\n",
    "D = numeric_only_iris(data)\n",
    "print(f\"Iris dataset shape: {D.shape}\")\n",
    "D.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6ee80-74f3-4299-8fea-515e6539eb68",
   "metadata": {},
   "source": [
    "We can see that $D$, has a 4D shape which we want to convert to 2D without loosing much information. So as mentioned, to begin standardizing the data we have the first get the `mean`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e83bb9-e578-4a81-ad1c-a3cba3375d2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.1 Calculating the mean\n",
    "\n",
    "The mean is the center of our data. The formula for it is:\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n - 1}\\sum_{i=1}^{n}x_i$$\n",
    "\n",
    "Which in our case is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a9430-d529-4ed5-b308-73dd7966eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f94f8a-d621-44c8-89b9-c633e17a03d7",
   "metadata": {},
   "source": [
    "#### 2.1.2 Calculating the variance\n",
    "Variance means the distance between variables and the mean. The formula for it is:\n",
    "$$S^{2}(x) = \\frac{1}{n - 1} \\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n",
    "\n",
    "While we are here we can see the formula for covariance. Covariance tells how the values of two variables change in relation to each other. The covariance *matrix* tells us the covariance of every variable with every other. Formula:\n",
    "\n",
    "$$\\text{cov}(x, y) = \\frac{1}{n - 1} \\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "The range of the function is $Cov(x, y) \\in [-1, 1]$. Generally the output number tells us if $x$ and $y$ rise together (if it's >0), if one rises and the other falls (<0) or if they don't jointly vary (=0).\n",
    "\n",
    "> We will see the value of the covariance matrix in the next point.\n",
    "\n",
    "\n",
    "#### 2.1.3 Calculating standard deviation\n",
    "Finally for std deviation we have:\n",
    "$$S(x) = \\sqrt{S^{2}(x)}$$\n",
    "\n",
    "$S$ tells us how far a variable can be from the mean ($S(x)$ can be >0 or <0). We find that in the above plot the standard deviation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33521b0-da09-4ad5-9e86-849630cdd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acca82-ffb9-4a68-9aaf-36d20e4e0165",
   "metadata": {},
   "source": [
    "> The difference between $S^2$ and $S$ is that $S^2$ is the average squared distance from the mean and $S$ is, of course, the square root of the distance.\n",
    "\n",
    "And will use `zscoring` to standarize everything:\n",
    "$$z = \\frac{{x - \\bar{x}}}{{S(x)}}$$\n",
    "Let's see the first 5 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6ac76-f794-4ca4-8044-c849ab2eb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = standartize_data(D)\n",
    "D.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b87d8-64ea-4197-9e83-852cd55571eb",
   "metadata": {},
   "source": [
    "_Great!_ Now we can get the covariance matrix of the **standardized** variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964567a-9ca5-42de-b2c1-c42157be1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = cov(D)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea1eeb-0835-4e29-bb9f-d612188a42a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Eigenvalues and eigenvectors of the covariance matrix\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./Media/bench1.jpg\" width=\"40%\" height=\"20%\">\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "Let's take a step back and put aside the calculations we did for a second so we can give an explanation what are these *eigen* thingies.\n",
    "\n",
    "An eigenvector ($v$) is a vector that doesn't change its *span* when going through transformation, let's say of the matrix $A$. Eigenvalue ($\\lambda$) is a scalar value that tells us how much the eigenvector has been scaled. We can define it as:\n",
    "\n",
    "$$A v = \\lambda v$$\n",
    "\n",
    "This formula is really cool because we see that the product of $v$ and the transformation matrix $A$ equals the *same* vector, but **scaled** by $\\lambda$. I think this animation shows it nicely (the red and green vectors don't change their span as the yellow one does, these are the eigenvectors):\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./Media/eigen.gif\" width=\"40%\" height=\"20%\">\n",
    "</div>\n",
    "\n",
    "> Generally when we use a transformation matrix on a vector we get new vector that has been multiplied by the transformation matrix and it looks like: $A v = v'$. But here the vector remains the same :o\n",
    "\n",
    "Now we know what are eigenvectors and eigenvalues let's put them in use. It turns out that when we calculate the eigenvectors of a given covariance matrix, the vector that has the **largest eigenvalue** tells us the direction with the biggest variance (information)[<sup>\\[2\\]</sup>](#fn2)! Second biggest eigenvector is the one *orthogonal* to the first. \n",
    "\n",
    "Now is also a good time to mention that in PCA, the eigenvalues of the covariance matrix of the dataset are also called *principal components* (PCs).\n",
    "\n",
    "Let's see this in our plot. We will:\n",
    "1. Calculate the eigenvectors with `np.linalg.eig`\n",
    "2. Sort them from highest to lowest,\n",
    "3. Create separate variables for the vectors and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a8aaf-75d2-43e3-a1f9-54af7dc797e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate eigen- values and vectors of the cov. matrix Z\n",
    "eigen_values, eigen_vectors = np.linalg.eig(Z)\n",
    "\n",
    "max_abs_idx = np.argmax(np.abs(eigen_vectors), axis=0)\n",
    "signs = np.sign(eigen_vectors[max_abs_idx, range(eigen_vectors.shape[0])])\n",
    "eigen_vectors = eigen_vectors*signs[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae4cdb-dfc7-4a45-b595-ea9c4a073c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigen_vectors = eigen_vectors.T\n",
    "\n",
    "# Create a list of tuples with every eigenvector and its eigenvalue\n",
    "eigen_pairs = [(np.abs(eigen_values[i]), eigen_vectors[i,:]) for i in range(len(eigen_values))]\n",
    "\n",
    "# Sort values to raise accuracy\n",
    "eigen_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Set new variables for the sorted eigen values and vectors for future use\n",
    "eigen_vals_sorted = np.array([x[0] for x in eigen_pairs])\n",
    "eigen_vecs_sorted = np.array([x[1] for x in eigen_pairs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bd428-44b8-421c-bdd8-30bef82694bc",
   "metadata": {},
   "source": [
    "Congrats. Now that we have the eigen- values and vectors of $\\text{cov}(D)$ we have everything to start the dimensionality reduction process.\n",
    "\n",
    "For starters we need to transform out dataset $D_{m\\times k}$ into a matrix $X_{k\\times n}$ where $n < k$. We would also need a projection matrix $P$ in which every row is an eigenvector of the covariance matrix of $D$. We would use these to calculate the dot product of $D$ and $P$. The expression should look like this:\n",
    "\n",
    "$$X_{m\\times n} = D_{m\\times k} P_{m\\times k}^T$$\n",
    "\n",
    "> Note: Here projection and transformation look pretty similar. In short, projection is a transformation in which we reduce the dimension. Think about it - when we put a **3D** ball in front of a lamp the ball projects a **2D** image, a *shadow*. For more information check these publications (they are also in the bibliography)\": \n",
    "- Shlens, Jonathon. A Tutorial on Principal Component Analysis, Google Research, Mountain View, CA 94043. Link: https://arxiv.org/pdf/1404.1100.pdf\n",
    "- Xu, Yang. 10-701 Machine Learning (Spring 2012) - Principal Component Analysis. Link: https://www.cs.cmu.edu/~tom/10601_fall2012/slides/pca.pdf\n",
    "\n",
    "Aaaand this should be it. Let's see it in action! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc5213-255d-4564-ac25-d1edf53d56c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Projection\n",
    "# R - num of dimensions\n",
    "R = 2\n",
    "P = eigen_vecs_sorted[:R]\n",
    "X = D @ P.T\n",
    "\n",
    "features = len(P.T)\n",
    "\n",
    "print(f\"Original shape matrix D: {D.shape}\")\n",
    "print(f\"Our new matrix: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2063d76-5455-4fff-ba01-c2e0d9f65952",
   "metadata": {},
   "source": [
    "*Awesome.* Now for the fun part - plotting! Let's see how much our PCs explain our variance by summing up all the eigenvalues and dividing them per PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3ebd2-abea-4a19-98b3-d22a23bc75c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the variance that is explained by every single PC\n",
    "total_eigen_values = sum(eigen_vals_sorted)\n",
    "variance_per_pc = []\n",
    "\n",
    "for i in eigen_vals_sorted:\n",
    "    k = i / total_eigen_values\n",
    "    variance_per_pc.append(k)\n",
    "\n",
    "# Print the variance per PC\n",
    "print(f\"Variance per principal compenent:\")\n",
    "\n",
    "for i in range(len(eigen_vecs_sorted)):   \n",
    "    print(f\"PC{i + 1}: {variance_per_pc[i] * 100:.2f}%\")\n",
    "\n",
    "# Set x and y to number of components and percentage explained variance correspondently\n",
    "x = np.arange(1, features + 1)\n",
    "y = np.cumsum(variance_per_pc)\n",
    "\n",
    "# Make the plot more clear to read and set labels and colors\n",
    "plt.xticks(np.arange(1, features+1))\n",
    "plt.xlabel(\"PC #\")\n",
    "plt.ylabel(\"% Explained variance\")\n",
    "plt.title(\"Cumalative sum of the explained variace\")\n",
    "\n",
    "# I hardcoded the colors cause I wanted these exact ones\n",
    "plt.bar(x, y, color = [\"#FFC300\", \n",
    "                       \"#FF5733\", \n",
    "                       \"#C70039\", \n",
    "                       \"#900C3F\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef16cb-9e29-4d0c-b69b-304bcb5eb622",
   "metadata": {},
   "source": [
    "Now let's see the projected variables onto a scatter plot so we can visualize the data. We will replace the names of the plants with numbers to see which species is where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad3740-8f7a-4122-a6aa-11c024c9416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Set the numbers for the species\n",
    "species_mapping = {\"setosa\": 0, \"versicolor\": 1, \"virginica\": 2}\n",
    "\n",
    "# Replace the species names with numbers\n",
    "data_copy[\"species\"] = data_copy[\"species\"].replace(species_mapping)\n",
    "\n",
    "# Reverse the species mapping dictionary to get species names from numbers\n",
    "species_names = {v: k for k, v in species_mapping.items()}\n",
    "\n",
    "# Plot with legend. Here I use sns cause I find it 10x easier to plot the variables with their representing colors\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(x=X[0], y=X[1], hue=data_copy[\"species\"], palette=\"Set1\")\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(f\"2 components, captures {y[1]:.2f} of total variation\")\n",
    "\n",
    "# Get the handles and labels of the legend\n",
    "handles, _ = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Modify the legend labels using species names\n",
    "legend_labels = [species_names[int(label)] for label in _]\n",
    "\n",
    "plt.legend(handles, legend_labels, title=\"Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e7427-3d8e-42ab-ae55-c5ccc6759c4e",
   "metadata": {},
   "source": [
    "Now we can visualize the 4D data into 2D. This is useful when we want to see what the data tells us. In this case we can see that `setosa` differs a lot from the two other species. This wouldn't be so clearly seen just from reading the raw data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f5de1-e18e-4c9b-b0ac-29f05fc98ded",
   "metadata": {},
   "source": [
    "## 3. Some other examples of PCA\n",
    "Now we will look at more interesting cases where we can extrapolate some meaningful information!\n",
    "Here is the list of the datasets we will look at:\n",
    "- Breast cancer dataset\n",
    "- Eigenfaces\n",
    "\n",
    "> From now on I will use sklearn for PCA because I find it more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a4873-1119-41d3-92aa-b11b62f080c4",
   "metadata": {},
   "source": [
    "#### 3.1 Breast cancer dataset\n",
    "\n",
    "This is a dataset from the `sklearn` library. We will name it $D_{\\text{cancer}}$ so to not get it confused with the first example when we write our code. The set contains 30 features and 569 samples from different people. Let's take a quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7f590-620c-4081-97f3-b37cadf2025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_ds = load_breast_cancer()\n",
    "D_cancer = pd.DataFrame(data = cancer_ds.data, columns = cancer_ds.feature_names)\n",
    "D_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0dfc6a-296b-4ee2-bdb4-5fd4dfdc1033",
   "metadata": {},
   "source": [
    "Let's set our goal to transform $D_{\\text{cancer}}$ from 30D to 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f4c0c-a887-4095-94e8-f4372230da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "D_cancer_std = StandardScaler().fit_transform(D_cancer)\n",
    "\n",
    "# Put PCA into a variable for convenience\n",
    "pca = PCA(n_components = 3).fit(D_cancer_std)\n",
    "\n",
    "# Perform PCA\n",
    "D_cancer_pca = pca.transform(D_cancer_std)\n",
    "\n",
    "# Calculate explained variance\n",
    "cum_sum_exp_ratio = np.cumsum(pca.explained_variance_ratio_ * 100)\n",
    "\n",
    "print(f\"Original shape of D_cancer: {D_cancer.shape}\")\n",
    "print(f\"Shape of the new dataset: {D_cancer_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72516294-ed9b-4f15-8f67-c51a99b015a3",
   "metadata": {},
   "source": [
    "From 30 to 3, nice. Now we will plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3484d5-cd06-4780-bf43-b72275bcafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.arange(1, D_cancer_pca.shape[1] + 1)\n",
    "y1 = cum_sum_exp_ratio\n",
    "\n",
    "print(\"Variance per principal compenent:\")\n",
    "for i in range(len(pca.explained_variance_ratio_)):\n",
    "    print(f\"PC{i + 1}: {pca.explained_variance_ratio_[i] * 100:.2f}%\")\n",
    "\n",
    "\n",
    "plt.bar(x1, y1, color = 'orange')\n",
    "plt.xlabel('PC #')\n",
    "plt.ylabel('% Explained variance')\n",
    "plt.title(\"Cumalative sum of the explained variance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd552ec0-c419-4ba4-969d-27a0acc65e2a",
   "metadata": {},
   "source": [
    "From the graph we see that with 3 components we only can explain ~70% of the information. If we want we can increase the number of dimensions to get above 90% explained variance, but the current result will suffice.\n",
    "\n",
    "Now let's see the 2D plot. It is useful to know that `load_breast_cancer()` has `.target_names` attribute that can tell us which cancers are benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c735d8-0956-4bdc-a658-9229adc47cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique features\n",
    "unique_features = np.unique(cancer_ds.target)\n",
    "\n",
    "# Set up a color map for the features\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_features)))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=[6,4])\n",
    "\n",
    "for feature, color in zip(unique_features, colors):\n",
    "    indices = np.where(cancer_ds.target == feature)\n",
    "    plt.scatter(D_cancer_pca[indices, 0], D_cancer_pca[indices, 1], color=color)\n",
    "\n",
    "# Add legend and plot the grapgh\n",
    "legend_labels = cancer_ds.target_names\n",
    "plt.legend(legend_labels)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"2D Projection of the 30D dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29778f2-0dfb-4bd0-8911-d051e5e970f2",
   "metadata": {},
   "source": [
    "We can see that the benign tumors are clustered together. This can suggest that the benign tumors don't vary that much and tend to have similar shape and size, since the variables aren't as spread as the malignant ones. The latter shows us that there is much more variance in the size and shape. Let's see the 3D graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd1569-3799-4cbb-87d3-b667184ff70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "fig = plt.figure(figsize = (7, 7))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "for feature, color in zip(unique_features, colors):\n",
    "    indices = np.where(cancer_ds.target == feature)\n",
    "    ax.scatter(D_cancer_pca[indices, 0], D_cancer_pca[indices, 1], D_cancer_pca[indices, 2], color=color)\n",
    "\n",
    "# Add legend\n",
    "legend_labels = cancer_ds.target_names\n",
    "ax.legend(legend_labels)\n",
    "plt.title(\"3D Projection of the 30D dataset\")\n",
    "ax.set_xlabel(\"PC2\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae6ea1-8f4a-465f-84b8-c96ed43a8ae3",
   "metadata": {},
   "source": [
    "The 3D graph gives us a better view of the clustering, but something else might catch the eye. There are some really distant points and we can check which of those are outliers. \n",
    "\n",
    "There are many tools for detecting outliers. One simple one is the already mentioned `zscore` which measures the distance from the mean. Let's plot the outliers using this method.\n",
    "\n",
    "> As mentioned, there are many really cool ways to detect an outlier. Another cool way to do that is to use *mahalanobis* distance[<sup>\\[4\\]</sup>](#fn4)[<sup>\\[5\\]</sup>](#fn5), which calculates the distance from point $a$ to the *distribution* of a given set.\n",
    "\n",
    "\n",
    "We will begin by creating a function `outlierz()` (hehe) that gets matrix X as input and performs zscore on it. The outliers_indexes uses `np.where` to find the indexes for the outliers and returns a list with all of them. `thresh` is manually set and is the threshold that zscore number has to exceed to be considered an outlier (usually equals 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4576e-c087-4fb8-8df8-ecd0137f3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlierz(X, thresh = 3):\n",
    "    '''\n",
    "    Takes X as input. \n",
    "    thresh > 3 generally means outlier\n",
    "    returns list with all of the outliers\n",
    "    '''\n",
    "    z_scores = stats.zscore(X, axis = 0)\n",
    "    outliers_indexies = np.where(z_scores > thresh) \n",
    "    \n",
    "    return X[outliers_indexies[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0086e8-b241-49b4-b589-0d81b342d072",
   "metadata": {},
   "source": [
    "Let's try it on a 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd5f6d-ae58-4697-9423-1024558ca89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "plt.figure(figsize=[6,4])\n",
    "\n",
    "for feature, color in zip(unique_features, colors):\n",
    "    indices = np.where(cancer_ds.target == feature)\n",
    "    plt.scatter(D_cancer_pca[indices, 0], D_cancer_pca[indices, 1], color=color)\n",
    "    \n",
    "\n",
    "# Get and plot the outliers\n",
    "outliers = outlierz(D_cancer_pca, 3)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], color=\"lime\", marker='x', label='Outliers')\n",
    "\n",
    "# Add legend and plot the graph\n",
    "legend_labels = np.append(cancer_ds.target_names, 'Outliers')\n",
    "plt.legend(legend_labels)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"2D Projection of the 30D dataset with Outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856f764-3dcc-4e25-a00f-84949411f1e4",
   "metadata": {},
   "source": [
    "*Hmm*. These outliers are strange. Many of them we weren't expecting to be so close to the center. What's up with that? \n",
    "\n",
    "Maybe a 3D plot will give us a better picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169d02f-f7b5-4e6a-80b7-93bb424b27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "fig = plt.figure(figsize=[6, 6])\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for feature, color in zip(unique_features, colors):\n",
    "    indices = np.where(cancer_ds.target == feature)\n",
    "    ax.scatter(D_cancer_pca[indices, 0], D_cancer_pca[indices, 1], D_cancer_pca[indices, 2], color=color)\n",
    "\n",
    "# Get and plot the outliers\n",
    "outliers = outlierz(D_cancer_pca, 3)\n",
    "ax.scatter(outliers[:, 0], outliers[:, 1], outliers[:, 2], color=\"lime\", marker='x', label='Outliers')\n",
    "\n",
    "# Add legend and plot the graph\n",
    "legend_labels = np.append(cancer_ds.target_names, 'Outliers')\n",
    "ax.legend(legend_labels)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.set_title(\"3D Projection of the 30D dataset with Outliers\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c682340-cb32-4d08-9adc-53db7c222f51",
   "metadata": {},
   "source": [
    "*Aha!* It looks like those outliers were behind the cluster of normal data. At least that's what I think it is. \n",
    " \n",
    " > Side note: As I was looking into this I came across the fact that as I increase the value of `n_components` (how many eigenvectors we want), some of the points become outliers. At least when using zscore. Further testing is needed to see if with other methods we will observe the same behavior. For now I think it is simply because of loss of information when we reduce the dimensions, since the first 2-3 PCs capture ~90% of the data (of course it depends on the data, as we saw in this example, the explain variance was less that 90%). It could be that those remaining percents are the reason for the new categorization of variables as outliers, but as mentioned - further tests needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d43d76-3d97-4525-960e-be02466c0763",
   "metadata": {},
   "source": [
    "#### 3.2 Eigenfaces\n",
    "PCA Can also be used to form a basis matrix for faces, this is how the method is used to classify images of celebrities. Similar is the method we can use to reconstruct the original faces from the basis matrix which looks really cool. Let's see how we can do that.\n",
    "\n",
    "We will use the tried-and-true olivetti dataset. The goal is to reconstruct the original faces with the basis matrix. Along the way we will see the \"mean face\", the face we will get when we get the mean of the dataset.\n",
    "\n",
    "The workflow is almost exactly the same as with the other examples. Only difference is that to reconstruct a face we use `.inverse_transform` which scales the data back to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dd00b-4741-465c-a235-604647651127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Olivetti faces dataset\n",
    "dataset_faces = fetch_olivetti_faces()\n",
    "faces = dataset_faces.data\n",
    "target = dataset_faces.target\n",
    "image_shape_faces = dataset_faces.images[0].shape\n",
    "\n",
    "# Perform PCA on the faces dataset\n",
    "pca_faces = PCA(n_components=100)  \n",
    "pca_result_faces = pca_faces.fit_transform(faces)\n",
    "\n",
    "# Reconstruct the faces using the principal components\n",
    "reconstructed_faces = pca_faces.inverse_transform(pca_result_faces)\n",
    "\n",
    "plt.imshow(pca_faces.mean_.reshape(image_shape_faces), cmap=\"gray\")\n",
    "\n",
    "plt.title(\"Mean face\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af7c27-04f5-46fb-be96-d9a155b97e4f",
   "metadata": {},
   "source": [
    "Looks... like it's made of wax. Anyways let's see the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82320f7e-4dcb-4de0-8084-b181b40dc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and reconstructed faces\n",
    "# Number of faces to display\n",
    "n_faces = 2  \n",
    "fig, axes = plt.subplots(n_faces, 2, figsize=(5, 5))\n",
    "for i in range(n_faces):\n",
    "    axes[i, 0].imshow(faces[i].reshape(image_shape_faces), cmap='gray')\n",
    "    axes[i, 0].set_title('Original Face')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(reconstructed_faces[i].reshape(image_shape_faces), cmap='gray')\n",
    "    axes[i, 1].set_title('Reconstructed Face')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1ee6b-373c-465b-9338-35e7ad3fb729",
   "metadata": {},
   "source": [
    "*It works!* We can get better results when we increase `n_components` if we want.\n",
    "\n",
    "What about objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed29b8f-7381-4628-9ca2-ba9d0785dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an image of a Babylonian clay tablet with mathematical formulas on it\n",
    "image_path = \"./Media/Babylonian-tablet.jpg\"\n",
    "image = np.array(Image.open(image_path).convert('L'))\n",
    "\n",
    "# Plot\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de2627-34da-4e03-8bce-8e12905cf5d6",
   "metadata": {},
   "source": [
    "Now let's put it through PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d522ad-2a59-477f-8c26-7eba4030e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(5, 5))\n",
    "\n",
    "pc_values = [5, 15, 45, 135]  # Values for the number of principal components\n",
    "\n",
    "for i, pc in enumerate(pc_values):\n",
    "    # Put PCA into a variable for convenience\n",
    "    pca = PCA(n_components=pc).fit(image)\n",
    "    # Perform PCA\n",
    "    image_pca = pca.transform(image)\n",
    "    recon_image = pca.inverse_transform(image_pca)\n",
    "    \n",
    "    row = i // 2  # Determine the row index\n",
    "    col = i % 2  # Determine the column index\n",
    "    \n",
    "    axes[row, col].imshow(recon_image, cmap='gray')\n",
    "    axes[row, col].set_title(f'PC = {pc}')\n",
    "\n",
    "plt.tight_layout()  # Adjust the spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfd0bd-89da-409a-9984-c983808191ac",
   "metadata": {},
   "source": [
    "Now let's create a function that puts an image through PCA and plots so to see some more examples just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff17924-a7b8-4bfd-871a-c9a56d79c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_print(images, pc_values, s = 5):\n",
    "    '''\n",
    "    Takes to values and prints a table with images:\n",
    "    images - list of image dirs to print\n",
    "    pc_values - list with ints that we use for different scales of PCA\n",
    "    s - size of plot, default = 5 but I suggest >= 8\n",
    "    '''\n",
    "    for m in images:\n",
    "        # Get image\n",
    "        image = np.array(Image.open(m).convert('L'))\n",
    "        \n",
    "        # Set plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(s, s))\n",
    "        \n",
    "        # Put image through PCA\n",
    "        for i, pc in enumerate(pc_values):\n",
    "            # Put PCA into a variable for convenience\n",
    "            pca = PCA(n_components=pc).fit(image)\n",
    "            # Perform PCA\n",
    "            image_pca = pca.transform(image)\n",
    "            recon_image = pca.inverse_transform(image_pca)\n",
    "\n",
    "            row = i // 2  # Determine the row index\n",
    "            col = i % 2  # Determine the column index\n",
    "            \n",
    "            axes[row, col].imshow(recon_image, cmap=\"gray\")\n",
    "            axes[row, col].set_title(f\"PC = {pc}\")\n",
    "            \n",
    "        # Plot\n",
    "        fig.suptitle(m.split(\"/\")[-1].split(\".jpg\")[0], fontsize=16)\n",
    "        plt.tight_layout()  \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610144e-5348-4b1f-8b1e-7d0176abdd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of .jpg directories. If for some reason they don't work they can be found\n",
    "# in /Project/Media\n",
    "\n",
    "img_data = [\"./Media/Babylonian-tablet.jpg\",\n",
    "           \"./Media/bench1.jpg\",\n",
    "           \"./Media/banana.jpg\",\n",
    "           \"./Media/fourier.jpg\",\n",
    "           \"./Media/mLisa.jpg\"]\n",
    "\n",
    "pc_values = [1, 5, 15, 135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c442e-3dba-41ec-abae-49c56efc00db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigen_print(img_data, pc_values, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef2e53-0fdc-444d-9cd6-5612e76d023e",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "That's it folks! I am fully aware that the work here in this project is nothing new (some might say even bloated), but for me the process was really fun and I had joy understanding and explaining the stuff I have learned. I believe tools such as ML (and AI) in which category PCA falls into can be extremely helpful to people, not only for compressing images and better visualization of data, but to actually get a better view of the world piece by piece. My plans for the future is to get an even deeper understanding of the matter at hand and use it for cool, meaningful things (and maybe upload this somewhere so to help other hopeless data science students)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeee0be-d247-4962-a0f2-59599f85276f",
   "metadata": {},
   "source": [
    "## 5. References:\n",
    "    \n",
    "   <span id=\"fn1\"> 1. Types of PCA. Aiasprirant. Link: https://aiaspirant.com/types-of-pca/#:~:text=Types%20of%20PCA%20%7C%20Kernel%20PCA,PCA%20%7C%20Incremental%20PCA%20in%20Python. </span>\n",
    "    \n",
    "    \n",
    "   <span id=\"fn2\">2. Spryut, Vincent. A geometric interpretation of the covariance matrix. Pg4. Link: https://users.cs.utah.edu/~tch/CS4640/resources/A%20geometric%20interpretation%20of%20the%20covariance%20matrix.pdf  </span>\n",
    "   \n",
    "   \n",
    "   <span id=\"fn3\"> 3. Wikipedia. Standard score. Link: https://en.wikipedia.org/wiki/Standard_score#</span>\n",
    "   \n",
    "   <span id=\"fn4\"> 4. Prabhakaran, Selva. Mahalanobis Distance â€“ Understanding the math with examples (python). Link: https://www.machinelearningplus.com/statistics/mahalanobis-distance/</span>\n",
    "   \n",
    "   <span id=\"fn5\"> 5. Wikipedia. Mahalanobis distance. Link: https://en.wikipedia.org/wiki/Mahalanobis_distance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac9867-97a5-4e54-98e2-c3cd00daeecf",
   "metadata": {},
   "source": [
    "## 6. Bibliography: \n",
    "\n",
    "1. Spryut, Vincent. A geometric interpretation of the covariance matrix. Pg4.  Link: https://users.cs.utah.edu/~tch/CS4640/resources/A%20geometric%20interpretation%20of%20the%20covariance%20matrix.pdf.\n",
    "\n",
    "2. Shlens, Jonathon. A Tutorial on Principal Component Analysis, Google Research, Mountain View, CA 94043. Link: https://arxiv.org/pdf/1404.1100.pdf\n",
    "3. Xu, Yang. 10-701 Machine Learning (Spring 2012) - Principal Component Analysis. Link: https://www.cs.cmu.edu/~tom/10601_fall2012/slides/pca.pdf\n",
    "\n",
    "2. Bagheri, Alireza. Principal Component Analysis (PCA) from Scratch. Link: https://bagheri365.github.io/blog/Principal-Component-Analysis-from-Scratch/#Step_5\n",
    "\n",
    "3. Holland, Steven (5 December, 2019). PRINCIPAL COMPONENTS ANALYSIS (PCA), Department of Geology, University of Georgia, Athens, GA 30602-2501. Link: https://strata.uga.edu/8370/handouts/pcaTutorial.pdf\n",
    "\n",
    "4. (Couldn't find author). 3.6.10.14. The eigenfaces example: chaining PCA and SVMs, scipy-lectures.org. Link: https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_eigenfaces.html\n",
    "\n",
    "4. Jaadi, Zakaria. Step-by-Step Explanation of Principal Component Analysis (PCA). Link: https://builtin.com/data-science/step-step-explanation-principal-component-analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
